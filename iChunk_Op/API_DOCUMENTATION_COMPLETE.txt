================================================================================
                    CHUNKING OPTIMIZER API - COMPLETE DOCUMENTATION
                    Enterprise-Grade Data Processing & Vector Search System
================================================================================

EXECUTIVE SUMMARY
================================================================================

The Chunking Optimizer API is a powerful, enterprise-grade system designed for 
processing large datasets (up to 3GB+), intelligent text chunking, embedding 
generation, and semantic search with advanced metadata filtering capabilities.

KEY CAPABILITIES:
- 4 Processing Modes: Fast, Config-1, Deep Config (9-step pipeline), Campaign
- Large File Support: Handle CSV files up to 3GB+ with streaming I/O
- Multiple Chunking Strategies: 7 different methods for optimal data organization
- Vector Storage: FAISS and ChromaDB support with metadata indexing
- Database Integration: Direct MySQL and PostgreSQL import
- OpenAI Compatible: Drop-in replacement for OpenAI embeddings API
- Smart Retrieval: Advanced company matching and contextual search
- Export Options: CSV, NumPy (.npy), JSON formats

PERFORMANCE METRICS:
- Fast Mode: ~60 seconds for 100K rows
- Parallel Processing: 6 workers for embedding generation
- Memory Efficient: 2K row batches for large file processing
- Turbo Mode: Optimized processing with parallel execution

USE CASES:
- Document processing and semantic search
- Media campaign data analysis
- Contact/lead management systems
- Knowledge base creation
- Data preprocessing for ML pipelines
- Enterprise search solutions

================================================================================
SYSTEM ARCHITECTURE & DATA FLOW
================================================================================

ARCHITECTURE OVERVIEW:
┌─────────────────────────────────────────────────────────────────────────────┐
│                           CLIENT LAYER                                      │
│   Web UI (Streamlit) | Python SDK | cURL | REST Clients | Mobile Apps      │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │ HTTP/REST API
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                        FASTAPI SERVER                                       │
│                    (Port 8001 - main.py)                                   │
│  ┌─────────────┐ ┌─────────────┐ ┌──────────────┐ ┌─────────────────────┐ │
│  │  Fast Mode  │ │  Config-1   │ │ Deep Config  │ │   Campaign Mode     │ │
│  │ Endpoints   │ │ Endpoints   │ │ (9 Steps)    │ │   Endpoints         │ │
│  └─────────────┘ └─────────────┘ └──────────────┘ └─────────────────────┘ │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      PROCESSING ENGINE                                      │
│                    (backend.py + backend_campaign.py)                       │
│  ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌─────────────────────────┐ │
│  │Preprocess  │ │ Chunking   │ │ Embedding  │ │    Storage & Retrieval  │ │
│  │ Pipeline   │ │Strategies  │ │Generation  │ │                         │ │
│  └────────────┘ └────────────┘ └────────────┘ └─────────────────────────┘ │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
         ┌────────────────────┼────────────────────┐
         │                    │                    │
         ▼                    ▼                    ▼
  ┌──────────────┐     ┌─────────────┐      ┌─────────────────┐
  │ DATA SOURCES │     │ VECTOR DBs  │      │ EXPORT FORMATS  │
  │ • CSV (3GB+) │     │ • FAISS     │      │ • CSV           │
  │ • MySQL      │     │ • ChromaDB  │      │ • NumPy (.npy)  │
  │ • PostgreSQL │     │ • Metadata  │      │ • JSON          │
  └──────────────┘     └─────────────┘      └─────────────────┘

DATA FLOW PROCESS:
1. INPUT: File upload or database connection
2. PREPROCESSING: Text cleaning, type conversion, null handling
3. CHUNKING: Intelligent text segmentation using selected strategy
4. EMBEDDING: Vector generation using local or OpenAI models
5. STORAGE: Index creation in FAISS or ChromaDB with metadata
6. RETRIEVAL: Semantic search with optional metadata filtering
7. EXPORT: Results in multiple formats (CSV, JSON, NumPy)

================================================================================
PROCESSING MODES
================================================================================

1. FAST MODE (Automatic Processing)
   Purpose: Quick prototyping, demos, exploration
   Processing Time: ~60 seconds for 100K rows
   
   Features:
   - Single API call processing
   - Automatic preprocessing (HTML removal, lowercase, whitespace normalization)
   - Semantic clustering using KMeans
   - Default paraphrase-MiniLM-L6-v2 model
   - FAISS storage with cosine similarity
   
   Best For: Initial data exploration, quick results, testing

2. CONFIG-1 MODE (Configurable Processing)
   Purpose: Production applications, custom requirements
   Processing Time: ~90 seconds for 100K rows
   
   Features:
   - Custom chunking strategies (fixed, recursive, semantic, document)
   - Model selection (local or OpenAI)
   - Storage choice (FAISS or ChromaDB)
   - Retrieval metric selection (cosine, euclidean, dot product)
   - Document-based chunking with key columns
   
   Best For: Production deployments, specific requirements

3. DEEP CONFIG MODE (Advanced 9-Step Pipeline)
   Purpose: Enterprise applications, maximum data quality control
   Processing Time: ~120 seconds for 100K rows
   
   Step-by-Step Process:
   1. PREPROCESS: Load data and basic cleaning
   2. TYPE_CONVERT: Convert column data types
   3. NULL_HANDLE: Handle null values with 7 strategies
   4. DUPLICATES: Remove duplicates with 4 strategies
   5. STOPWORDS: Remove stop words (optional)
   6. NORMALIZE: Text normalization (lemmatize/stem)
   7. CHUNK: Advanced chunking with metadata extraction
   8. EMBED: Parallel embedding generation
   9. STORE: Enhanced storage with metadata indexing
   
   Features:
   - Maximum control over each processing step
   - Metadata extraction and filtering
   - Statistical aggregations (mean, median, mode)
   - Advanced text processing
   - Filtered search capabilities
   
   Best For: Enterprise applications, data quality critical scenarios

4. CAMPAIGN MODE (Specialized for Media Campaigns)
   Purpose: Media campaigns, contact data, lead management
   Processing Time: ~75 seconds for 100K rows
   
   Features:
   - Smart field detection and mapping
   - 5 specialized chunking methods:
     * Record-based: Group contact records by count
     * Company-based: Group by company name
     * Source-based: Group by lead source
     * Semantic clustering: Intelligent grouping
     * Document-based: Group by key column
   - Smart company retrieval (2-stage matching)
   - Complete record preservation
   - Contextual column display
   
   Best For: CRM systems, lead management, campaign analysis

================================================================================
API ENDPOINTS - COMPLETE REFERENCE
================================================================================

CORE PROCESSING ENDPOINTS
================================================================================

1. FAST MODE PROCESSING
   Endpoint: POST /run_fast
   Purpose: Automatic processing with minimal configuration
   
   Parameters:
   - file: CSV file upload (optional)
   - db_type: Database type (mysql, postgresql, sqlite)
   - host: Database host (optional)
   - port: Database port (optional)
   - username: Database username (optional)
   - password: Database password (optional)
   - database: Database name (optional)
   - table_name: Table name (optional)
   - process_large_files: Enable large file processing (true/false)
   - use_turbo: Enable turbo mode (true/false)
   - batch_size: Embedding batch size (default: 256)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/run_fast \
     -F "file=@data.csv" \
     -F "use_turbo=true" \
     -F "batch_size=256"
   
   Response:
   {
     "mode": "fast",
     "summary": {
       "rows": 1000,
       "chunks": 50,
       "stored": "faiss",
       "embedding_model": "paraphrase-MiniLM-L6-v2",
       "retrieval_ready": true,
       "turbo_mode": true
     }
   }

2. CONFIG-1 MODE PROCESSING
   Endpoint: POST /run_config1
   Purpose: Configurable processing with custom parameters
   
   Parameters:
   - file: CSV file upload (optional)
   - db_type: Database type (optional)
   - host, port, username, password, database, table_name: Database connection
   - chunk_method: Chunking strategy (fixed, recursive, semantic, document)
   - chunk_size: Chunk size in characters (default: 400)
   - overlap: Overlap between chunks (default: 50)
   - n_clusters: Number of clusters for semantic chunking (default: 10)
   - document_key_column: Key column for document chunking (optional)
   - token_limit: Token limit for document chunking (default: 2000)
   - retrieval_metric: Similarity metric (cosine, euclidean, dot)
   - model_choice: Embedding model (paraphrase-MiniLM-L6-v2, all-MiniLM-L6-v2, text-embedding-ada-002)
   - storage_choice: Storage backend (faiss, chroma)
   - apply_default_preprocessing: Apply default preprocessing (true/false)
   - process_large_files: Enable large file processing (true/false)
   - use_turbo: Enable turbo mode (true/false)
   - batch_size: Embedding batch size (default: 256)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/run_config1 \
     -F "file=@data.csv" \
     -F "chunk_method=document" \
     -F "model_choice=all-MiniLM-L6-v2" \
     -F "storage_choice=faiss" \
     -F "use_turbo=true"
   
   Response:
   {
     "mode": "config1",
     "summary": {
       "rows": 1000,
       "chunks": 45,
       "stored": "faiss",
       "embedding_model": "all-MiniLM-L6-v2",
       "retrieval_ready": true,
       "turbo_mode": true
     }
   }

3. DEEP CONFIG MODE - STEP-BY-STEP PROCESSING
   
   Step 1: Preprocess
   Endpoint: POST /deep_config/preprocess
   Purpose: Load and clean data
   
   Parameters:
   - file: CSV file upload (optional)
   - db_type, host, port, username, password, database, table_name: Database connection
   
   Response:
   {
     "status": "success",
     "rows": 1000,
     "columns": 10,
     "column_names": ["col1", "col2", ...],
     "data_types": {"col1": "object", "col2": "int64", ...},
     "file_info": {...}
   }
   
   Step 2: Type Convert
   Endpoint: POST /deep_config/type_convert
   Purpose: Convert column data types
   
   Parameters:
   - type_conversions: JSON string of column type mappings
   
   Example:
   {
     "age": "integer",
     "salary": "float",
     "date_joined": "datetime",
     "is_active": "boolean"
   }
   
   Step 3: Null Handle
   Endpoint: POST /deep_config/null_handle
   Purpose: Handle null values
   
   Parameters:
   - null_strategies: JSON string of null handling strategies
   
   Example:
   {
     "age": "mean",
     "name": "unknown",
     "salary": "median",
     "department": "mode"
   }
   
   Step 4: Duplicates
   Endpoint: POST /deep_config/duplicates
   Purpose: Remove duplicate rows
   
   Parameters:
   - strategy: Duplicate removal strategy (keep_first, keep_last, remove_all, keep_all)
   
   Step 5: Stopwords
   Endpoint: POST /deep_config/stopwords
   Purpose: Remove stop words
   
   Parameters:
   - remove_stopwords: Enable stopword removal (true/false)
   
   Step 6: Normalize
   Endpoint: POST /deep_config/normalize
   Purpose: Text normalization
   
   Parameters:
   - text_processing: Processing method (none, lemmatize, stem)
   
   Step 7: Chunk
   Endpoint: POST /deep_config/chunk
   Purpose: Create text chunks with metadata
   
   Parameters:
   - chunk_method: Chunking method (fixed, recursive, semantic, document)
   - chunk_size: Chunk size (default: 400)
   - overlap: Overlap size (default: 50)
   - key_column: Key column for document chunking (optional)
   - token_limit: Token limit (default: 2000)
   - n_clusters: Number of clusters (default: 10)
   - store_metadata: Enable metadata storage (true/false)
   - selected_numeric_columns: JSON array of numeric columns for metadata
   - selected_categorical_columns: JSON array of categorical columns for metadata
   
   Step 8: Embed
   Endpoint: POST /deep_config/embed
   Purpose: Generate embeddings
   
   Parameters:
   - model_name: Embedding model (default: paraphrase-MiniLM-L6-v2)
   - batch_size: Batch size (default: 64)
   - use_parallel: Enable parallel processing (true/false)
   
   Step 9: Store
   Endpoint: POST /deep_config/store
   Purpose: Store embeddings in vector database
   
   Parameters:
   - storage_type: Storage backend (chroma, faiss)
   - collection_name: Collection name (default: deep_config_collection)

CAMPAIGN MODE ENDPOINTS
================================================================================

1. CAMPAIGN PROCESSING
   Endpoint: POST /campaign/run
   Purpose: Process media campaign data with specialized features
   
   Parameters:
   - file: CSV file upload (optional)
   - db_type, host, port, username, password, database, table_name: Database connection
   - chunk_method: Campaign chunking method (record_based, company_based, source_based, semantic_clustering, document_based)
   - chunk_size: Records per chunk (default: 5)
   - model_choice: Embedding model (default: paraphrase-MiniLM-L6-v2)
   - storage_choice: Storage backend (faiss, chroma)
   - use_openai: Use OpenAI API (true/false)
   - openai_api_key: OpenAI API key (optional)
   - openai_base_url: Custom OpenAI base URL (optional)
   - process_large_files: Enable large file processing (true/false)
   - use_turbo: Enable turbo mode (true/false)
   - batch_size: Batch size (default: 256)
   - preserve_record_structure: Preserve record structure (true/false)
   - document_key_column: Key column for document chunking (optional)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/campaign/run \
     -F "file=@campaign_data.csv" \
     -F "chunk_method=company_based" \
     -F "model_choice=paraphrase-MiniLM-L6-v2" \
     -F "use_turbo=true"
   
   Response:
   {
     "mode": "campaign",
     "summary": {
       "rows": 1000,
       "chunks": 200,
       "stored": "faiss",
       "embedding_model": "paraphrase-MiniLM-L6-v2",
       "retrieval_ready": true,
       "turbo_mode": true,
       "media_campaign_processed": true,
       "field_mapping": {
         "company_name": "company_name",
         "lead_source": "lead_source",
         "email": "email"
       },
       "chunk_method": "company_based"
     }
   }

2. CAMPAIGN RETRIEVAL
   Endpoint: POST /campaign/retrieve
   Purpose: Standard campaign data retrieval
   
   Parameters:
   - query: Search query
   - search_field: Search field (all, company, email, etc.)
   - k: Number of results (default: 5)
   - include_complete_records: Include complete records (true/false)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/campaign/retrieve \
     -F "query=Microsoft" \
     -F "search_field=company" \
     -F "k=5" \
     -F "include_complete_records=true"
   
   Response:
   {
     "query": "Microsoft",
     "search_field": "company",
     "k": 5,
     "include_complete_records": true,
     "results": [
       {
         "rank": 1,
         "content": "COMPANY CONTACTS: Microsoft...",
         "similarity": 0.95,
         "distance": 0.05,
         "metric": "cosine",
         "complete_record": [
           {
             "company_name": "Microsoft Corporation",
             "contact_name": "John Doe",
             "email": "john.doe@microsoft.com",
             "phone": "+1-555-0123"
           }
         ]
       }
     ],
     "retrieval_method": "semantic_search",
     "processing_time": "0.15s"
   }

3. SMART COMPANY RETRIEVAL
   Endpoint: POST /campaign/smart_retrieval
   Purpose: Two-stage intelligent company matching
   
   Parameters:
   - query: Company search query
   - search_field: Search field (auto, company, all)
   - k: Number of results (default: 5)
   - include_complete_records: Include complete records (true/false)
   
   Features:
   - Stage 1: Exact and partial company name matching
   - Stage 2: Semantic fallback if no company matches found
   - Intelligent field detection and mapping
   - Similarity scoring for partial matches
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/campaign/smart_retrieval \
     -F "query=Microsoft Corp" \
     -F "search_field=auto" \
     -F "k=5"
   
   Response:
   {
     "query": "Microsoft Corp",
     "search_field": "company_auto",
     "results": [
       {
         "rank": 1,
         "content": "COMPANY MATCH: Microsoft Corporation (EXACT)...",
         "similarity": 1.0,
         "match_type": "exact",
         "company_matched": "Microsoft Corporation",
         "matched_field": "company_name",
         "complete_record": [...]
       }
     ],
     "total_matches": 3,
     "exact_matches": 1,
     "partial_matches": 2,
     "retrieval_method": "company_keyword_matching",
     "processing_time": "0.08s"
   }

RETRIEVAL & SEARCH ENDPOINTS
================================================================================

1. BASIC RETRIEVAL
   Endpoint: POST /retrieve
   Purpose: Standard semantic search
   
   Parameters:
   - query: Search query
   - k: Number of results (default: 5)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/retrieve \
     -F "query=artificial intelligence" \
     -F "k=5"
   
   Response:
   {
     "query": "artificial intelligence",
     "k": 5,
     "results": [
       {
         "rank": 1,
         "content": "AI and machine learning technologies...",
         "similarity": 0.89,
         "distance": 0.11
       }
     ]
   }

2. METADATA FILTERED RETRIEVAL
   Endpoint: POST /retrieve_with_metadata
   Purpose: Advanced search with metadata filtering
   
   Parameters:
   - query: Search query
   - k: Number of results (default: 5)
   - metadata_filter: JSON string of metadata filters
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/retrieve_with_metadata \
     -F "query=machine learning" \
     -F "k=5" \
     -F "metadata_filter={\"department\":\"engineering\",\"year\":\"2023\"}"
   
   Response:
   {
     "status": "success",
     "query": "machine learning",
     "results": [
       {
         "rank": 1,
         "content": "ML algorithms in engineering...",
         "similarity": 0.92,
         "distance": 0.08,
         "metadata": {
           "department": "engineering",
           "year": "2023",
           "chunk_id": "fixed_0001"
         }
       }
     ],
     "total_results": 3,
     "metadata_filter_applied": true,
     "store_type": "faiss"
   }

3. OPENAI-COMPATIBLE RETRIEVAL
   Endpoint: POST /v1/retrieve
   Purpose: OpenAI-style retrieval endpoint
   
   Parameters:
   - query: Search query
   - model: Model name (default: all-MiniLM-L6-v2)
   - n_results: Number of results (default: 5)
   
   Response Format:
   {
     "object": "list",
     "data": [
       {
         "object": "retrieval_result",
         "score": 0.89,
         "content": "AI and machine learning...",
         "rank": 1
       }
     ],
     "model": "all-MiniLM-L6-v2",
     "query": "artificial intelligence",
     "n_results": 5
   }

DATABASE INTEGRATION ENDPOINTS
================================================================================

1. DATABASE CONNECTION TEST
   Endpoint: POST /db/test_connection
   Purpose: Test database connectivity
   
   Parameters:
   - db_type: Database type (mysql, postgresql)
   - host: Database host
   - port: Database port
   - username: Database username
   - password: Database password
   - database: Database name
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/db/test_connection \
     -F "db_type=mysql" \
     -F "host=localhost" \
     -F "port=3306" \
     -F "username=user" \
     -F "password=pass" \
     -F "database=mydb"
   
   Response:
   {
     "status": "success"
   }

2. LIST DATABASE TABLES
   Endpoint: POST /db/list_tables
   Purpose: Get list of tables in database
   
   Parameters: Same as test_connection
   
   Response:
   {
     "tables": ["users", "orders", "products", "campaigns"]
   }

3. DATABASE IMPORT & PROCESS
   Endpoint: POST /db/import_one
   Purpose: Import table and process in one step
   
   Parameters:
   - db_type, host, port, username, password, database: Database connection
   - table_name: Table to import
   - processing_mode: Processing mode (fast, config1, deep)
   - use_turbo: Enable turbo mode (true/false)
   - batch_size: Batch size (default: 256)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/db/import_one \
     -F "db_type=mysql" \
     -F "host=localhost" \
     -F "port=3306" \
     -F "username=user" \
     -F "password=pass" \
     -F "database=mydb" \
     -F "table_name=campaigns" \
     -F "processing_mode=fast" \
     -F "use_turbo=true"

EXPORT ENDPOINTS
================================================================================

1. EXPORT CHUNKS
   Endpoint: GET /export/chunks
   Purpose: Export processed chunks as CSV
   
   Response: CSV file download
   
   Example Request:
   curl http://127.0.0.1:8001/export/chunks --output chunks.csv

2. EXPORT EMBEDDINGS (NUMPY)
   Endpoint: GET /export/embeddings
   Purpose: Export embeddings as NumPy array
   
   Response: .npy file download
   
   Example Request:
   curl http://127.0.0.1:8001/export/embeddings --output embeddings.npy

3. EXPORT EMBEDDINGS (JSON)
   Endpoint: GET /export/embeddings_text
   Purpose: Export embeddings as JSON
   
   Response: JSON file download
   
   Example Request:
   curl http://127.0.0.1:8001/export/embeddings_text --output embeddings.json

4. EXPORT PREPROCESSED DATA
   Endpoint: GET /export/preprocessed
   Purpose: Export preprocessed data as CSV
   
   Response: CSV file download

5. DEEP CONFIG EXPORTS
   Endpoints:
   - GET /deep_config/export/preprocessed
   - GET /deep_config/export/chunks
   - GET /deep_config/export/embeddings
   
   Purpose: Export data from deep config processing steps

6. CAMPAIGN EXPORTS
   Endpoints:
   - GET /campaign/export/chunks
   - GET /campaign/export/embeddings
   - GET /campaign/export/preprocessed
   
   Purpose: Export campaign-specific data

SYSTEM & MONITORING ENDPOINTS
================================================================================

1. ROOT ENDPOINT
   Endpoint: GET /
   Purpose: API information and status
   
   Response:
   {
     "message": "Chunking Optimizer API is running",
     "version": "1.0",
     "large_file_support": true,
     "max_recommended_file_size": "3GB+",
     "openai_compatible": true,
     "performance_optimized": true,
     "embedding_batch_size": 256,
     "parallel_workers": 6
   }

2. HEALTH CHECK
   Endpoint: GET /health
   Purpose: System health status
   
   Response:
   {
     "status": "healthy",
     "large_file_support": true,
     "performance_optimized": true
   }

3. SYSTEM INFORMATION
   Endpoint: GET /system_info
   Purpose: System resource information
   
   Response:
   {
     "memory_usage": "45%",
     "available_memory": "8.5 GB",
     "total_memory": "16.0 GB",
     "large_file_support": true,
     "max_recommended_file_size": "3GB+",
     "embedding_batch_size": 256,
     "parallel_workers": 6
   }

4. FILE INFORMATION
   Endpoint: GET /file_info
   Purpose: Current file processing information
   
   Response:
   {
     "filename": "data.csv",
     "file_size": 1048576,
     "upload_time": "2024-01-15T10:30:00",
     "location": "Temporary storage"
   }

5. CAPABILITIES
   Endpoint: GET /capabilities
   Purpose: System capabilities and features
   
   Response:
   {
     "openai_compatible_endpoints": [
       "/v1/embeddings",
       "/v1/chat/completions",
       "/v1/retrieve"
     ],
     "processing_modes": ["fast", "config1", "deep", "campaign"],
     "large_file_support": true,
     "max_file_size_recommendation": "3GB+",
     "supported_embedding_models": [
       "all-MiniLM-L6-v2",
       "paraphrase-MiniLM-L6-v2",
       "text-embedding-ada-002"
     ],
     "chunking_methods": {
       "fast": ["semantic_clustering"],
       "config1": ["fixed_size", "recursive", "document_based", "semantic_clustering"],
       "deep": ["fixed_size", "recursive", "document_based", "semantic_clustering"],
       "campaign": ["record_based", "company_based", "source_based", "semantic_clustering", "document_based"]
     },
     "batch_processing": true,
     "memory_optimized": true,
     "database_large_table_support": true,
     "performance_features": {
       "turbo_mode": true,
       "parallel_processing": true,
       "optimized_batch_size": 256,
       "caching_system": true
     },
     "campaign_features": {
       "smart_company_retrieval": true,
       "field_detection": true,
       "contextual_display": true,
       "complete_records": true,
       "specialized_preprocessing": true
     }
   }

OPENAI-COMPATIBLE ENDPOINTS
================================================================================

1. EMBEDDINGS API
   Endpoint: POST /v1/embeddings
   Purpose: OpenAI-compatible embeddings generation
   
   Parameters:
   - model: Model name (text-embedding-ada-002, etc.)
   - input: Text or array of texts to embed
   - openai_api_key: OpenAI API key (optional)
   - openai_base_url: Custom base URL (optional)
   
   Example Request:
   curl -X POST http://127.0.0.1:8001/v1/embeddings \
     -F "model=text-embedding-ada-002" \
     -F "input=Hello world" \
     -F "openai_api_key=your-key-here"
   
   Response:
   {
     "object": "list",
     "data": [
       {
         "object": "embedding",
         "embedding": [0.1, 0.2, 0.3, ...],
         "index": 0
       }
     ],
     "model": "text-embedding-ada-002",
     "usage": {
       "prompt_tokens": 2,
       "total_tokens": 2
     }
   }

2. CHAT COMPLETIONS API
   Endpoint: POST /v1/chat/completions
   Purpose: OpenAI-compatible chat completions (requires external OpenAI API)
   
   Parameters:
   - model: Model name (gpt-3.5-turbo, gpt-4, etc.)
   - messages: Array of message objects
   - max_tokens: Maximum tokens (default: 1000)
   - temperature: Temperature (default: 0.7)
   - openai_api_key: OpenAI API key
   - openai_base_url: Custom base URL (optional)

UNIVERSAL PROCESSING ENDPOINT
================================================================================

1. UNIVERSAL PROCESSOR
   Endpoint: POST /api/v1/process
   Purpose: Single endpoint for all operations
   
   Parameters:
   - operation: Operation type (fast, config1, deep_config, deep_config_step, retrieve, export, system, db_test, db_list, db_import)
   - [All other parameters based on operation type]
   
   This endpoint consolidates all operations into a single interface for easier integration.

================================================================================
ADVANCED FEATURES
================================================================================

METADATA FILTERING SYSTEM
================================================================================

The system supports advanced metadata filtering for precise search results:

1. METADATA EXTRACTION
   - Automatic column type detection (numeric, categorical)
   - Smart cardinality filtering (excludes high-cardinality text columns)
   - Statistical aggregations (mean, median, mode, min, max)
   - Categorical mode values per chunk

2. FILTERING CAPABILITIES
   - Exact value matching
   - Range filtering for numeric values
   - Multiple condition filtering
   - Fast indexed search using metadata index

3. USAGE EXAMPLE
   {
     "department": "engineering",
     "year": 2023,
     "salary_min": 50000,
     "salary_max": 100000
   }

SMART COMPANY RETRIEVAL
================================================================================

Campaign Mode includes intelligent company matching:

1. TWO-STAGE PROCESS
   Stage 1: Exact and partial company name matching
   - Exact matches: Case-insensitive exact string matching
   - Partial matches: Contains-based matching with similarity scoring
   - Field detection: Automatic identification of company columns
   
   Stage 2: Semantic fallback
   - If no company matches found, falls back to semantic search
   - Uses embedding-based similarity
   - Maintains consistent result structure

2. FEATURES
   - Automatic field mapping and detection
   - Similarity scoring for partial matches
   - Complete record preservation
   - Contextual display of matched fields

LARGE FILE HANDLING
================================================================================

The system is optimized for processing large files (up to 3GB+):

1. STREAMING PROCESSING
   - Direct file streaming to disk (no memory loading)
   - Batch processing in configurable chunks
   - Memory-efficient processing pipeline
   - Automatic garbage collection

2. PERFORMANCE OPTIMIZATIONS
   - Parallel processing with 6 workers
   - Optimized batch sizes (256 for embeddings)
   - Turbo mode for faster processing
   - Caching system for repeated operations

3. MEMORY MANAGEMENT
   - Dynamic memory usage monitoring
   - Automatic batch size adjustment
   - Safe memory limits (80% of available)
   - Large table detection for databases

BATCH PROCESSING
================================================================================

Support for batch operations:

1. BATCH RETRIEVAL
   Endpoint: POST /batch_retrieve (Campaign Mode)
   - Process multiple queries in single request
   - JSON array of queries with individual parameters
   - Consistent result structure for all queries

2. BATCH EMBEDDING
   - Process multiple texts in single API call
   - Optimized for large-scale embedding generation
   - Parallel processing support

================================================================================
ERROR HANDLING & TROUBLESHOOTING
================================================================================

COMMON ERROR RESPONSES
================================================================================

1. VALIDATION ERRORS
   Status Code: 400 Bad Request
   Response:
   {
     "error": "Invalid parameters provided",
     "details": "Missing required parameter: file"
   }

2. PROCESSING ERRORS
   Status Code: 500 Internal Server Error
   Response:
   {
     "error": "Processing failed",
     "details": "Out of memory during embedding generation"
   }

3. DATABASE ERRORS
   Status Code: 500 Internal Server Error
   Response:
   {
     "error": "Database connection failed",
     "details": "Connection timeout to MySQL server"
   }

4. FILE ERRORS
   Status Code: 400 Bad Request
   Response:
   {
     "error": "File processing failed",
     "details": "Unsupported file format. Only CSV files are supported."
   }

TROUBLESHOOTING GUIDE
================================================================================

1. MEMORY ISSUES
   Problem: "Out of memory" errors
   Solutions:
   - Reduce batch_size parameter
   - Enable use_turbo=true for optimization
   - Process smaller files or use database import
   - Check system resources with /system_info

2. SLOW PROCESSING
   Problem: Processing takes too long
   Solutions:
   - Enable use_turbo=true
   - Use Fast Mode for initial testing
   - Reduce chunk_size for faster chunking
   - Check system resources and available memory

3. DATABASE CONNECTION ISSUES
   Problem: Cannot connect to database
   Solutions:
   - Verify connection parameters
   - Test connection with /db/test_connection
   - Check database server status
   - Verify network connectivity

4. IMPORT ERRORS
   Problem: Package import failures
   Solutions:
   - Install requirements: pip install -r requirements.txt
   - Download spaCy model: python -m spacy download en_core_web_sm
   - Check Python version compatibility
   - Verify virtual environment activation

5. PORT CONFLICTS
   Problem: Port 8001 already in use
   Solutions:
   - Change port in main.py: uvicorn.run(app, host="127.0.0.1", port=8002)
   - Kill existing process using the port
   - Use different port for multiple instances

PERFORMANCE OPTIMIZATION TIPS
================================================================================

1. FOR LARGE FILES (1GB+)
   - Always enable use_turbo=true
   - Use process_large_files=true
   - Consider database import instead of file upload
   - Monitor memory usage with /system_info

2. FOR PRODUCTION DEPLOYMENTS
   - Use Config-1 or Deep Config modes for better control
   - Enable parallel processing
   - Use FAISS for faster retrieval
   - Implement proper error handling and retries

3. FOR CAMPAIGN DATA
   - Use Campaign Mode for specialized features
   - Enable smart_company_retrieval for better results
   - Use company_based chunking for contact data
   - Preserve record structure for complete data access

================================================================================
INTEGRATION EXAMPLES
================================================================================

PYTHON INTEGRATION
================================================================================

```python
import requests
import json

# Initialize API client
API_BASE = "http://127.0.0.1:8001"

class ChunkingOptimizerAPI:
    def __init__(self, base_url=API_BASE):
        self.base_url = base_url
    
    def process_file_fast(self, file_path, use_turbo=True):
        """Process file using Fast Mode"""
        with open(file_path, 'rb') as f:
            response = requests.post(
                f"{self.base_url}/run_fast",
                files={'file': f},
                data={'use_turbo': str(use_turbo).lower()}
            )
        return response.json()
    
    def search(self, query, k=5):
        """Search processed data"""
        response = requests.post(
            f"{self.base_url}/retrieve",
            data={'query': query, 'k': k}
        )
        return response.json()
    
    def export_chunks(self, output_path):
        """Export chunks to CSV"""
        response = requests.get(f"{self.base_url}/export/chunks")
        with open(output_path, 'wb') as f:
            f.write(response.content)
        return True

# Usage example
api = ChunkingOptimizerAPI()

# Process file
result = api.process_file_fast("data.csv", use_turbo=True)
print(f"Processed {result['summary']['rows']} rows into {result['summary']['chunks']} chunks")

# Search
results = api.search("artificial intelligence", k=5)
for item in results['results']:
    print(f"Rank {item['rank']}: {item['content'][:100]}...")
    print(f"Similarity: {item['similarity']:.3f}\n")

# Export
api.export_chunks("results.csv")
```

CURL EXAMPLES
================================================================================

```bash
# 1. Health check
curl http://127.0.0.1:8001/health

# 2. Process file with Fast Mode
curl -X POST http://127.0.0.1:8001/run_fast \
  -F "file=@data.csv" \
  -F "use_turbo=true"

# 3. Process with Config-1 Mode
curl -X POST http://127.0.0.1:8001/run_config1 \
  -F "file=@data.csv" \
  -F "chunk_method=document" \
  -F "model_choice=all-MiniLM-L6-v2" \
  -F "storage_choice=faiss" \
  -F "use_turbo=true"

# 4. Campaign Mode processing
curl -X POST http://127.0.0.1:8001/campaign/run \
  -F "file=@campaign_data.csv" \
  -F "chunk_method=company_based" \
  -F "use_turbo=true"

# 5. Search
curl -X POST http://127.0.0.1:8001/retrieve \
  -F "query=machine learning" \
  -F "k=5"

# 6. Smart company search
curl -X POST http://127.0.0.1:8001/campaign/smart_retrieval \
  -F "query=Microsoft" \
  -F "k=5"

# 7. Database import
curl -X POST http://127.0.0.1:8001/db/import_one \
  -F "db_type=mysql" \
  -F "host=localhost" \
  -F "port=3306" \
  -F "username=user" \
  -F "password=pass" \
  -F "database=mydb" \
  -F "table_name=campaigns" \
  -F "processing_mode=fast"

# 8. Export chunks
curl http://127.0.0.1:8001/export/chunks --output chunks.csv

# 9. System information
curl http://127.0.0.1:8001/system_info

# 10. OpenAI-compatible embeddings
curl -X POST http://127.0.0.1:8001/v1/embeddings \
  -F "model=text-embedding-ada-002" \
  -F "input=Hello world"
```

JAVASCRIPT/NODE.JS INTEGRATION
================================================================================

```javascript
const axios = require('axios');
const FormData = require('form-data');
const fs = require('fs');

class ChunkingOptimizerAPI {
    constructor(baseUrl = 'http://127.0.0.1:8001') {
        this.baseUrl = baseUrl;
    }
    
    async processFileFast(filePath, useTurbo = true) {
        const form = new FormData();
        form.append('file', fs.createReadStream(filePath));
        form.append('use_turbo', useTurbo.toString());
        
        const response = await axios.post(`${this.baseUrl}/run_fast`, form, {
            headers: form.getHeaders()
        });
        
        return response.data;
    }
    
    async search(query, k = 5) {
        const form = new FormData();
        form.append('query', query);
        form.append('k', k.toString());
        
        const response = await axios.post(`${this.baseUrl}/retrieve`, form, {
            headers: form.getHeaders()
        });
        
        return response.data;
    }
    
    async getSystemInfo() {
        const response = await axios.get(`${this.baseUrl}/system_info`);
        return response.data;
    }
}

// Usage example
async function main() {
    const api = new ChunkingOptimizerAPI();
    
    try {
        // Process file
        const result = await api.processFileFast('data.csv', true);
        console.log(`Processed ${result.summary.rows} rows`);
        
        // Search
        const searchResults = await api.search('artificial intelligence', 5);
        searchResults.results.forEach(item => {
            console.log(`Rank ${item.rank}: ${item.content.substring(0, 100)}...`);
            console.log(`Similarity: ${item.similarity.toFixed(3)}\n`);
        });
        
        // System info
        const systemInfo = await api.getSystemInfo();
        console.log(`Memory usage: ${systemInfo.memory_usage}`);
        
    } catch (error) {
        console.error('Error:', error.response?.data || error.message);
    }
}

main();
```

================================================================================
PERFORMANCE BENCHMARKS
================================================================================

PROCESSING TIMES BY DATASET SIZE
================================================================================

| Dataset Size | Fast Mode | Config-1 Mode | Deep Config Mode | Campaign Mode |
|--------------|-----------|---------------|------------------|---------------|
| 1K rows      | ~1s       | ~2s           | ~3s              | ~2s           |
| 10K rows     | ~6s       | ~9s           | ~12s             | ~7s           |
| 100K rows    | ~60s      | ~90s          | ~120s            | ~75s          |
| 1M rows      | ~10min    | ~15min        | ~20min           | ~12min        |
| 3GB file     | ~45min    | ~60min        | ~75min           | ~50min        |

MEMORY USAGE PATTERNS
================================================================================

| File Size | RAM Usage | Processing Mode | Notes |
|-----------|-----------|-----------------|-------|
| 100MB     | ~200MB    | All modes       | Efficient processing |
| 1GB       | ~2-3GB    | All modes       | Batch processing active |
| 3GB       | ~4-6GB    | All modes       | Streaming + optimization |
| 5GB+      | ~6-8GB    | Large file mode | Memory management critical |

OPTIMIZATION IMPACT
================================================================================

| Feature | Performance Gain | Use Case |
|---------|------------------|----------|
| Turbo Mode | 30-40% faster | All processing modes |
| Parallel Processing | 50-60% faster | Large datasets |
| FAISS Storage | 80% faster retrieval | High-frequency searches |
| Batch Processing | 70% less memory | Large files |
| Smart Caching | 90% faster repeats | Repeated operations |

================================================================================
SECURITY CONSIDERATIONS
================================================================================

1. API SECURITY
   - No authentication required by default (add for production)
   - Input validation on all endpoints
   - File size limits (configurable)
   - SQL injection protection for database connections

2. DATA SECURITY
   - Temporary files automatically cleaned up
   - No persistent storage of uploaded files
   - Database credentials not logged
   - Memory cleanup after processing

3. PRODUCTION DEPLOYMENT
   - Use HTTPS in production
   - Implement API key authentication
   - Set up rate limiting
   - Monitor resource usage
   - Regular security updates

================================================================================
CONCLUSION
================================================================================

The Chunking Optimizer API provides a comprehensive, enterprise-grade solution for 
data processing, intelligent chunking, and semantic search. With 40+ endpoints, 
4 processing modes, and advanced features like metadata filtering and smart 
company retrieval, it serves a wide range of use cases from simple document 
processing to complex campaign data analysis.

KEY STRENGTHS:
- Scalable architecture supporting files up to 3GB+
- Multiple processing modes for different use cases
- Advanced metadata filtering and search capabilities
- OpenAI-compatible API for easy integration
- Comprehensive export options
- Robust error handling and monitoring

The system is designed for both quick prototyping and production deployment, 
with performance optimizations that make it suitable for enterprise applications 
requiring high throughput and reliability.

For additional support, examples, and detailed technical documentation, refer to 
the API-DOCUMENTATION directory in the project repository.

================================================================================
END OF DOCUMENTATION
================================================================================
